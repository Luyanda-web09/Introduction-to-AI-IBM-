# Introduction-to-AI-IBM-overview of artificial intelligence (AI) and its evolution, defining it as augmented intelligence that enhances human capabilities.

Module 1

History of AI

AI's origins trace back to early computing tools like the abacus and calculators.
Key milestones include the Turing Test proposed by Alan Turing in the 1950s and the development of expert systems in the 1970s.
Types of AI

AI can be categorized based on strength:
Weak AI (narrow AI) is task-specific and cannot learn new tasks.
Strong AI (generalized AI) can perform diverse tasks and learn autonomously.
Super AI (conscious AI) would possess human-level consciousness, which is not yet achievable.
Learning Mechanisms

AI learns through methods such as supervised learning, unsupervised learning, and reinforcement learning.
The development of machine learning models relies on the data and algorithms provided by humans.
This summary encapsulates the key concepts of AI, its historical context, types, and learning processes.

The different forms of intelligence utilized during a daily commute, highlighting the roles of human, artificial, and augmented intelligence.

Human Intelligence

Involves the skills needed to operate a vehicle, such as steering and checking mirrors.
Relies on human capabilities like creativity and emotional understanding.
Artificial Intelligence

Refers to machines performing tasks that typically require human intelligence, such as reasoning and problem-solving.
Examples include self-driving features that operate without human input.
Augmented Intelligence

Combines human and machine efforts to enhance capabilities, such as driver assist features like collision detection.
Aims to improve human performance by providing support in various tasks.
Strengths Matrix

Machines excel at processing large amounts of data quickly and performing repetitive tasks accurately.
Humans are better at generalizing information, creativity, and emotional intelligence, which are crucial for tasks like customer service.
Conclusion

Both artificial and augmented intelligence have their strengths, and the integration of both is essential for optimal results in various tasks.

Introducing generative AI and its various applications across different sectors.

Understanding Generative AI

Generative AI creates new content from scratch, unlike traditional AI, which analyzes data and makes decisions.
It utilizes deep learning techniques and large datasets to generate diverse forms of content, including text, images, and videos.
Capabilities of Generative AI

It can perform natural language processing tasks using large language models (LLMs) for text generation, translation, and summarization.
Generative AI enhances machine learning models through data augmentation and can engage in human-like conversations.
Use Cases Across Industries

In marketing, it personalizes advertisements and social media content based on user behavior.
In healthcare, it aids in tailored treatments and simulates surgeries, while in education, it creates customized learning materials.

The differences between traditional AI and generative AI, highlighting the evolution of AI systems.

Traditional AI Framework

In traditional AI, data is stored in a repository, which includes various types of information like tables, images, and documents.
An analytics platform processes this data to build predictive models, which are then applied in applications to take actions, such as preventing customer churn.
Feedback Loop in Traditional AI

A feedback loop is essential for traditional AI, allowing the system to learn from past predictions and improve its models over time.
This process helps refine the AI's accuracy by learning from both successes and mistakes.
Generative AI Framework

Generative AI starts with vast amounts of external data, not limited to an organization's internal repository, and utilizes large language models (LLMs).
The prompting and tuning layer customizes these general models to specific business needs, while the application layer serves the AI's purpose, similar to traditional AI.
Overall, the architecture of generative AI is fundamentally different due to the scale of data and models involved, necessitating a new approach to AI development.

The current state of Artificial Intelligence (AI) and whether we have achieved true AI, particularly in the context of Artificial General Intelligence (AGI).

Understanding AI and AGI

AI is defined as the simulation of intelligent behavior in computers, but true AGI, which would match human intelligence, has not yet been achieved.
Examples like calculators and chess-playing computers illustrate that while they can perform specific tasks, they do not possess true intelligence.
Turing Test and Its Implications

The Turing Test is introduced as a measure of AI, where a computer's responses are indistinguishable from a human's in conversation.
Although some chatbots have reportedly passed the Turing Test, the bar for what constitutes true intelligence remains subjective and often feels low.
Current Developments and Future Prospects

The conversation highlights the rapid advancements in AI technology, suggesting we are closer to achieving AGI than ever before.
Despite progress, the speakers agree that we have not yet reached a point where AI can fully replicate human-like intelligence across diverse tasks.

The integration of Artificial Intelligence (AI) in daily life and its various applications.

Common Use Cases of AI

AI personalizes digital experiences, such as social media feeds and online shopping recommendations, by analyzing user preferences and behaviors.
Virtual assistants like Siri and Alexa automate tasks in smart homes, enhancing convenience and energy efficiency.
Benefits of AI in Daily Life

AI improves security through biometric authentication, fraud detection, and enhanced surveillance systems.
In healthcare, AI chatbots and wearables monitor health metrics and provide support, aiding in early diagnosis and personalized treatment.
Enhancements in User Experience

AI enhances smartphone cameras and navigation apps, optimizing photo quality and travel routes.
Text input is streamlined through AI algorithms that suggest words and correct errors, improving communication efficiency.

An overview of AI chatbots and smart assistants, focusing on their functionality, benefits, and applications across various industries.

Understanding AI Chatbots and Smart Assistants

AI chatbots are software programs that utilize advanced algorithms and natural language processing (NLP) to understand and respond to user queries.
They have evolved from simple rule-based systems to sophisticated AI-powered assistants capable of engaging in context-based conversations.
Benefits of AI Chatbots

They offer 24/7 availability, allowing users to access information anytime.
Chatbots can handle large volumes of inquiries, providing personalized services based on user preferences and past interactions.
Applications Across Industries

In customer service, chatbots manage routine inquiries and provide instant support.
In healthcare, they assist with appointment scheduling and medication reminders, while in education, they support language learning and tutoring.
Future Trends

The chatbot industry is moving towards more human-like interactions, utilizing sentiment analysis to gauge customer emotions.
There is potential for chatbots to automate internal tasks in businesses, enhancing efficiency and streamlining processes.

The functionality and benefits of chatbots in various business scenarios.

Understanding Chatbots

Chatbots like Flora and Bertie can handle customer inquiries without human intervention, allowing business owners to focus on their core tasks.
They can respond to simple questions (e.g., store hours) or assist with complex tasks (e.g., checking bank balances) by processing user input and retrieving information.
Interaction Methods

Users can interact with chatbots through various platforms, including websites, phone calls, and social media messaging.
Chatbots utilize natural language processing and artificial intelligence to understand and respond to user requests effectively.
Benefits of Using Chatbots

Time-saving: Chatbots automate customer interactions, freeing up time for business owners to focus on other responsibilities.
Simplicity: Building a chatbot is often straightforward and does not require extensive coding knowledge.
Quick deployment: Chatbots can be developed and launched rapidly, often within an hour, due to their cloud-based nature.

The applications and impacts of Artificial Intelligence (AI) across various industries.

AI in Manufacturing

AI enhances efficiency through robotics and automation, allowing human workers to focus on complex tasks.
AI-driven systems, like image recognition, ensure product quality and reduce waste in manufacturing processes.
AI in Healthcare

AI assists in medical imaging analysis, helping radiologists detect abnormalities in scans.
Predictive analytics using AI improves patient management by anticipating health risks and optimizing resource allocation.
AI in Finance and Retail

In finance, AI chatbots enhance customer service, while robo-advisors provide automated investment advice.
Retail benefits from AI through personalized recommendations and demand forecasting, leading to better inventory management and customer engagement.
Overall, AI is transforming industries by improving efficiency, enhancing decision-making, and reshaping customer experiences.

The applications and advancements of Generative AI across various domains.

Generative AI Overview

Generative AI enables machines to autonomously create new content, including images, videos, and stories.
Language is a key application area, with tools built on large language models (LLMs) that have evolved from text-only to multimodal capabilities.
Advancements in Generative AI Models

OpenAI's GPT models and Google's Palm and Gemini models have enhanced capabilities for processing text and images.
These models support various tasks, such as image captioning and video description, providing new tools for content creators.
Applications Across Industries

Generative AI is transforming visual arts, voice, music generation, and video creation, with tools like Stable Diffusion and AIVA.
Major companies like Google, Salesforce, and Adobe are integrating Generative AI into their products, enhancing functionalities and driving innovation.


the various applications of machine learning (ML) in everyday life, highlighting its significance and impact.

Applications of Machine Learning

Natural Language Processing (NLP): ML enables chatbots for customer service, handling text-based queries and routing customers to human representatives when necessary. Voice assistants like Siri and Alexa also utilize ML for understanding spoken commands.
Mobile Applications: Services like Spotify and LinkedIn use ML for personalized recommendations. Smartphones leverage ML for features like facial recognition and image classification, enhancing user experience.

Financial and Security Applications

Fraud Detection: Financial institutions employ ML to analyze credit card transactions, identifying fraudulent activities among millions of daily transactions.
Cybersecurity: ML models are trained to detect and respond to cyberattacks, enhancing security measures.
Healthcare and Marketing

Healthcare: ML assists in interpreting medical images, improving accuracy in detecting conditions like cancer, and allowing radiologists to focus on critical cases.
Marketing: The marketing and sales departments utilize ML for lead generation and personalized campaigns, leveraging existing models for targeted outreach.
Overall, the content emphasizes that while discussions about advanced AI continue, machine learning is already a vital part of our daily lives.

An overview of key concepts and applications of Artificial Intelligence (AI) and its various forms.

Types of AI

Weak (Narrow) AI: Designed for specific tasks within limited domains.
Strong (Generalized) AI: Capable of performing a wide range of tasks across different domains.
Generative AI

Definition: A type of AI that creates content in various formats, including text, images, and audio.
Applications: Used in marketing, healthcare, gaming, and education, showcasing its versatility.
Benefits of AI

Automation: Enhances efficiency in daily tasks through virtual assistants and smart devices.
Personalization: Provides tailored recommendations in streaming and e-commerce platforms, improving user experience.

Module 2

The concept of cognitive computing and its significance in various industries.

Cognitive Computing Overview

Cognitive computing mimics human cognitive processes like thinking, reasoning, and problem-solving.
It enhances learning by providing personalized recommendations based on individual performance.
Core Elements of Cognitive Computing

Perception: Involves gathering and interpreting data from various sources.
Learning: Utilizes machine learning algorithms to analyze data and extract meaningful insights.
Applications and Benefits

Cognitive computing improves decision-making and efficiency by analyzing large datasets.
It is applied in industries such as healthcare, finance, and customer service, with examples including IBM Watson and Amazon's Alexa.

The key terminologies and concepts related to artificial intelligence (AI).

Understanding AI

AI is a branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence, such as learning, reasoning, and problem-solving.
AI is categorized into three types: narrow AI (specific tasks), general AI (human-like cognitive abilities), and super AI (theoretical and surpassing human intelligence).
Machine Learning and Deep Learning

Machine learning is a subset of AI that uses algorithms to analyze data and make decisions without explicit programming, allowing machines to solve problems autonomously.
Deep learning, a specialized subset of machine learning, employs multi-layered neural networks to analyze complex data and simulate human decision-making.
Neural Networks

Neural networks are computational models inspired by the human brain, consisting of interconnected nodes organized in three layers: input, hidden, and output layers.
These networks process raw data, perform complex computations, and produce results, enabling AI systems to learn and improve continuously.

An overview of the fundamentals of machine learning, a key aspect of artificial intelligence (AI).

Types of Machine Learning

Supervised Learning: Involves training algorithms on labeled data to classify new data. The more samples provided, the more accurate the classifications become.
Unsupervised Learning: Utilizes unlabeled data to identify patterns and group similar data points. It is effective for clustering and anomaly detection.
Key Differences from Traditional Programming

Model Creation: Unlike traditional algorithms that follow fixed rules, machine learning models evolve by learning from data, allowing for continuous improvement and adaptation.
Decision-Making: Machine learning models determine rules based on data inputs rather than relying on pre-defined if-then-else statements.
Reinforcement Learning

This type of learning involves setting rules and constraints for the algorithm, which learns to achieve goals by maximizing rewards through trial and error, applicable in scenarios like game playing or navigation tasks.

An overview of core concepts in machine learning, focusing on different learning techniques and the training process of models.

Machine Learning Categories

Supervised Learning: Involves training models using labeled datasets to predict or classify new data points.
Unsupervised Learning: Focuses on discovering patterns in unlabeled data, often through clustering techniques.
Reinforcement Learning: Utilizes a reward system to guide the learning process, rewarding good actions and penalizing bad ones.
Supervised Learning Tasks

Regression: Estimates continuous values by analyzing the relationship between input features and output results.
Classification: Assigns discrete class labels based on input features, using various models like decision trees and logistic regression.
Model Training Process

Training Set: Used to train the algorithm by providing labeled examples.
Validation Set: Helps fine-tune the model and validate its performance.
Test Set: Contains unseen data to evaluate the model's effectiveness using metrics like accuracy and precision.

The fundamentals of deep learning, a specialized subset of machine learning.

Deep Learning Overview

Deep learning layers algorithms to create a neural network, mimicking the brain's structure and functionality.
It enables AI systems to learn continuously and improve from unstructured data like images, videos, and audio.
Neural Network Functionality

Deep learning algorithms process data through multiple layers, where each layer refines the output for the next.
Developers configure the number of layers and functions, training the model with annotated examples to detect patterns.
Applications and Advantages

Deep learning excels in tasks such as image captioning, voice recognition, facial recognition, medical imaging, and language translation.
Unlike traditional machine learning, deep learning algorithms improve performance as they are exposed to more data, making them suitable for complex applications like driverless cars.

understanding neural networks, which are computational models inspired by the human brain.

Neural Network Structure

Neural networks consist of interconnected nodes called neurons, organized into layers: an input layer, one or more hidden layers, and an output layer.
The input layer receives data, hidden layers process it using activation functions, and the output layer produces the final result.
Learning Process

Neural networks learn through training, involving a forward pass where data is processed, followed by error calculation and a backward pass to adjust internal parameters.
This process is repeated with various data sets until the network makes accurate predictions.
Types of Neural Networks

Various types include perceptron neural networks (simplest form), feed-forward neural networks (information flows in one direction), and deep feed-forward networks (multiple hidden layers).
Other types are convolutional neural networks (suitable for visual data) and recurrent neural networks (consider context over time).

The distinction between machine learning and deep learning using a relatable example of deciding whether to order pizza.

Understanding Machine Learning and Deep Learning

Deep learning is a subset of machine learning, which is itself a subfield of artificial intelligence (AI).
Machine learning uses structured labeled data for predictions, while deep learning can work with unstructured data.
Machine Learning Example: Pizza Decision

Inputs for the pizza decision include time saved, weight loss, and cost savings, represented as binary values (1 for yes, 0 for no).
Weights are assigned to each input to determine their importance in the decision-making process.
Deep Learning Characteristics

A neural network is considered "deep" if it has more than three layers, including input and output layers.
Deep learning can automatically identify features in data without human intervention, unlike classical machine learning, which relies on human-defined features.

Generative AI models, which are designed to mimic human creativity in generating various forms of content.

Generative AI Models Overview

Generative AI models learn from large datasets by recognizing patterns and trends.
They utilize machine learning and deep learning algorithms to create new content.
Types of Generative AI Models

Variational Autoencoders (VAEs) transform input data through encoding and decoding, with applications in image generation and anomaly detection.
Generative Adversarial Networks (GANs) consist of a generator and a discriminator, training together to produce realistic data, used in image synthesis and style transfer.
Autoregressive Models and Transformers

Autoregressive models generate data sequentially, predicting the next element based on previous ones, applicable in text and music generation.
Transformers are used in natural language processing, enabling effective text generation and cross-language translations.
Unimodal vs. Multimodal Models

Unimodal models process and generate outputs within the same modality, while multimodal models handle different modalities, like text and images.
Examples include GPT-3 (unimodal) for text and DALL-E (multimodal) for generating images from text descriptions.

An overview of large language models (LLMs) and their applications in business settings.

Foundation Models

LLMs are part of a broader category known as foundation models, which can be adapted for various tasks.
These models are trained on vast amounts of unstructured data, allowing them to generate text by predicting the next word in a sentence.
Generative AI and Tuning

Foundation models excel in generative tasks but can also be fine-tuned for specific natural language processing (NLP) tasks using a small amount of labeled data.
Prompting techniques can be used to apply these models to classification tasks, even with limited data.
Advantages and Disadvantages

Advantages include high performance due to extensive training data and productivity gains from requiring less labeled data for specific tasks.
Disadvantages involve high compute costs for training and running these models, as well as trustworthiness issues due to the unverified nature of the training data.
Applications Beyond Language

Foundation models are also being developed for other domains, such as computer vision and chemistry, showcasing their versatility in various fields.

An overview of key concepts in artificial intelligence (AI) and its subfields, including machine learning and deep learning.

Understanding AI and Its Subfields

AI simulates human intelligence in machines, enabling them to perform tasks that typically require human thinking.
Machine learning (ML) is a subfield of AI that focuses on algorithms allowing computers to learn from data without explicit programming.
Categories of Machine Learning

Supervised learning involves training models on labeled data to make predictions.
Unsupervised learning finds patterns in data without predefined labels, while reinforcement learning learns through interaction and feedback.
Deep Learning and Foundation Models

Deep learning is a subset of ML that uses artificial neural networks with multiple layers to handle complex data like images and natural language.
Foundation models are large-scale neural networks trained on vast datasets, serving as a base for various applications, allowing for fine-tuning for specific tasks.
Large Language Models and Generative AI

Large language models (LLMs) are a type of foundation model focused on processing and generating human-like text, capable of various language tasks.
Generative AI refers to models designed to create new content, leveraging the knowledge from foundation models to produce innovative outputs.

An overview of key concepts in natural language processing (NLP), speech technology, and computer vision.

Natural Language Processing (NLP)

NLP is a subset of artificial intelligence that enables computers to understand, interpret, and produce human language.
It uses machine learning and deep learning algorithms to analyze the semantic meaning of words by deconstructing sentences and understanding context.
Speech Technology

Speech-to-text (STT) technology converts spoken words into written text using neural networks, enabling applications like real-time transcription and voice commands.
Text-to-speech (TTS) technology synthesizes spoken words from written text, facilitating seamless human-machine interaction and applications like virtual assistants and translation services.
Computer Vision

Computer vision allows machines to interpret and understand visual data, analyzing images or videos to derive meaningful insights.
It has applications in various industries, such as retail for inventory management, manufacturing for quality control, and agriculture for precision farming.

The concept of Natural Language Processing (NLP) and its significance in artificial intelligence.

Understanding Natural Language Processing

NLP enables computers to comprehend and process human language, translating unstructured text into structured data.
The process involves two main components: Natural Language Understanding (NLU) and Natural Language Generation (NLG).
Use Cases of NLP

Machine Translation: NLP helps in translating languages while maintaining context, avoiding misinterpretations.
Virtual Assistants and Chatbots: NLP allows these tools to understand and respond to human commands in both spoken and written forms.
NLP Techniques and Tools

Tokenization: Breaking down text into manageable chunks or tokens.
Stemming and Lemmatization: Techniques to reduce words to their base forms, aiding in understanding their meanings in context.

The advancements and challenges in the field of self-driving cars, particularly focusing on the importance of computer vision and object detection.

Self-Driving Cars and Computer Vision

The development of self-driving vehicles has gained significant interest since early competitions in 2005.
Key research areas include 3D object detection, which involves identifying vehicles, pedestrians, and signs in driving environments.
Challenges of Human Vision

Human drivers face limitations in visual attention, making it difficult to monitor everything on the road.
Computer vision and sensors can enhance awareness and help prevent accidents by detecting various elements in the environment.
Future of Self-Driving Technology

Self-driving cars have the potential to transform transportation and society, allowing passengers to engage in other activities while traveling.
Despite the excitement surrounding self-driving technology, there are still many challenges to address before achieving fully autonomous vehicles.

The integration of AI, cloud computing, edge computing, and the Internet of Things (IoT) and their impact on everyday life.

Understanding IoT

IoT devices are physical objects connected to the Internet that collect and share data, such as fitness trackers and smart thermostats.
These devices send data to the cloud for storage and analysis, enabling real-time applications.
Cloud Computing Basics

Cloud computing allows users to access and store data and services over the Internet, rather than on local devices.
AI enhances cloud computing by analyzing data and automating tasks, improving user experience, such as filtering spam emails.
Edge Computing Explained

Edge computing processes data closer to its source, allowing for faster decision-making without relying solely on the cloud.
Examples include smart thermostats that can adjust temperature settings instantly and security cameras with built-in AI for real-time alerts.
Overall, the convergence of these technologies leads to smarter applications that enhance efficiency and connectivity in daily life.

An overview of key concepts in machine learning and artificial intelligence (AI).

Machine Learning Types

Supervised Learning: Uses labeled data to classify new data, improving accuracy with more data. It includes regression (estimating continuous values) and classification (discrete values).
Unsupervised Learning: Identifies patterns in unlabeled data, useful for clustering and anomaly detection.
Reinforcement Learning: Focuses on achieving goals within defined rules by maximizing rewards, applicable in scenarios like game playing.
Neural Networks and Deep Learning

Neural Networks: Composed of interconnected nodes in three layers (input, hidden, output). Types include perceptron, feed-forward, and convolutional networks.
Deep Learning: Utilizes multiple layers in neural networks for complex data analysis, enhancing tasks like natural language understanding and image recognition.
Generative AI Models

Variational Autoencoders (VAEs): Encode input data into a latent space and decode it to generate outputs.
Generative Adversarial Networks (GANs): Consist of a generator that creates data and a discriminator that evaluates it, fostering the generation of realistic data samples.

Module 3

An overview of AI agents, their characteristics, and applications in various fields.

Understanding AI Agents

AI agents are software programs that autonomously interact with their environment, process data, and perform tasks to achieve human-defined goals.
They can make decisions, solve problems, and adapt to new information without constant human assistance.
Key Characteristics of AI Agents

Social Ability: AI agents can communicate and collaborate with other agents and entities, as seen in healthcare chatbots.
Autonomy: They operate independently, like self-driving cars that navigate without human input.
Multi-Agent Systems

Multi-agent systems involve multiple agents working together to achieve individual and collective goals, facilitating distributed problem-solving and cooperative decision-making.
Applications include online marketplaces, robotic coordination, and traffic management, where agents optimize processes and enhance efficiency.

The evolution of AI agents and the transition from monolithic models to compound AI systems.

Understanding AI Agents

AI agents are designed to solve complex problems by integrating various components and systems.
Monolithic models are limited by their training data and are difficult to adapt for specific tasks.
Compound AI Systems

These systems combine models with programmatic components, allowing for more accurate and efficient problem-solving.
An example illustrates how integrating a model with a database can yield correct answers, unlike standalone models.
Agentic Approach

Large language models (LLMs) can take charge of the logic in compound AI systems, allowing for reasoning and planning.
The agentic approach enables LLMs to break down complex problems and utilize external tools to find solutions.
Capabilities of LLM Agents

LLM agents can reason, act using external tools, and access memory for personalized experiences.
Configuring agents can involve combining reasoning and action, leading to more effective problem-solving.
Applications and Future of AI Agents

The content emphasizes the importance of system design and agentic behavior in developing AI agents.
As AI technology advances, the integration of these systems will become more prevalent, enhancing their capabilities.

An overview of robotics and automation, focusing on how robots work and their integration with AI technologies.

Understanding Robotics

Robotics involves the design, construction, and operation of robots that can perform tasks autonomously or with assistance.
Key components of robots include sensors (for environmental data), actuators (for movement), and controllers (for decision-making).
AI Integration in Robotics

Robots utilize AI technologies like machine learning and natural

How AI is transforming business operations across various industries.

AI in Business Operations

AI automates repetitive tasks such as data entry and scheduling, allowing employees to concentrate on more strategic work.
Organizations utilize AI tools like chatbots for customer service, enhancing efficiency and reducing costs.
AI Applications Across Industries

In recruitment, AI analyzes resumes and interviews to expedite hiring processes, as seen with Hilton's AI chatbot.
AI platforms in accounting automate payroll and auditing, improving accuracy and reducing human error, exemplified by EY's use of AI in auditing.
AI for Data Analysis and Decision Making

AI swiftly analyzes large data volumes, identifying patterns that lead to better predictions and fewer errors.
In marketing, AI tools enhance sales through personalized recommendations, as demonstrated by Amazon's inventory management.
AI's Role in Innovation and Creativity

AI aids in predictive maintenance in manufacturing, preventing equipment failures, with BMW using AI for quality inspections.
In healthcare, AI accelerates drug discovery by analyzing medical data, as shown by IBM's healthcare technology.

The transformative impact of Generative AI on businesses and its practical applications across various contexts.

Generative AI in Business

Generative AI can create unique content, such as marketing materials and product descriptions, helping businesses overcome creative challenges and enhance audience engagement.
Companies like Vanguard have successfully used Generative AI to improve sales conversion rates by crafting emotionally resonant messages.
Data Analysis and Insights

Generative AI analyzes complex datasets to uncover hidden patterns, enabling businesses to derive actionable insights and save time.
For instance, Salesforce utilizes Generative AI to enhance customer data analysis, improving sales strategies and customer interactions.
Customer Service and Product Development

AI-powered chatbots streamline customer service by handling repetitive tasks and providing personalized responses, as seen with Sephora's beauty recommendations.
In product development, companies like Nike leverage Generative AI to generate multiple design variations, speeding up the prototyping process and fostering innovation.

the transformative impact of generative AI on society and business, drawing parallels to the historical significance of fire.

Understanding Generative AI

Generative AI differs from traditional AI by being flexible and capable of creating new content rather than just executing specific tasks.
Foundation models are trained using self-supervised learning, allowing them to adapt to various tasks without extensive manual data labeling.
Modes of AI Consumption

Embedded AI is integrated into off-the-shelf software, providing productivity benefits but lacking differentiation.
API calls allow businesses to leverage external AI services, but they come with concerns about data governance and long-term value extraction.
The platform model enables businesses to build tailored AI solutions, fostering unique value creation and control over data.
Value Creation and Governance

Foundation models can be enhanced with proprietary data, increasing their value over time.
Strong AI governance is essential to avoid issues like data leaks and incorrect outputs, emphasizing the need for careful data management.
The future of AI will involve multiple models and open-source contributions, promoting democratization and innovation in the field.

Introduction
Retrieval-augmented generation (RAG) is a cutting-edge approach in the field of natural language processing (NLP) that combines the strengths of retrieval-based and generation-based models. This hybrid method is particularly effective for tasks that require generating informative and contextually relevant text, such as question answering, dialogue systems, and content creation.

Objectives
After completing this reading, you will be able to:

Explain the working of the RAG model.
Describe how RAG addresses the limitations of generative AI models.
Explore the applications and use cases of RAG.
Retrieval-augmented generation (RAG)

RAG is a hybrid approach that combines generative models with retrieval-based methods. RAG works through a three-step process. First, given a query, the model retrieves relevant documents or pieces of information from a predefined corpus or database. Next, these retrieved documents are used to augment the input to the generative model, providing it with additional context. Finally, the generative model uses both the original query and the retrieved information to generate a response, ensuring that the output is both contextually rich and factually grounded. This process enhances the accuracy and relevance of the generated content, addressing many limitations of standalone generative models.

Limitations of the generative AI models

Generative AI models, such as GPT-3 or GPT-4, have several limitations. They can produce information that sounds plausible but is factually incorrect or fabricated, a phenomenon known as "hallucination." Additionally, these models have a fixed knowledge cutoff date, meaning they lack access to information beyond their last training update, making them outdated for generating current content. Generative models also have limited context windows, which makes them struggle with tasks requiring long-term context or detailed memory over extended conversations or documents. Furthermore, while they can generate fluent and coherent text, they might lack the depth and specificity required for certain queries, especially those needing detailed or specialized knowledge. Finally, generating high-quality text can be computationally expensive and slow, particularly for complex or long-form content.

How RAG addresses the limitations of generative AI models

RAG addresses the limitations of generative AI models quite effectively. By retrieving relevant documents or data from a corpus before generating a response, RAG models ground the generation process in factual and contextually accurate information, thereby reducing hallucination. Leveraging real-time retrieval from up-to-date databases allows RAG models to provide more current and relevant information, mitigating the issue of the knowledge cutoff. Retrieval mechanisms enhance contextual understanding by pulling relevant documents or chunks of text that extend the effective context window, maintaining coherence over longer interactions. The retrieval step also ensures that the generative model has access to specific and detailed information relevant to the query, enhancing the specificity and depth of the responses. Moreover, RAG models can be more efficient because the retrieval step narrows down the information space, enabling the generative component to focus on synthesizing relevant information rather than generating text from scratch.

RAG: Key components

Retrieval component:

Function: The retrieval component of RAG is responsible for searching and extracting relevant information from a large corpus of documents or a knowledge base. This step ensures that the model has access to factual and contextually appropriate information.

Mechanism: Typically, this involves using a retriever model like BM25 or dense retrievers based on neural networks to find the most relevant passages or documents that match a given query.

Generation component:

Function: The generation component takes the retrieved information and uses it to generate coherent and contextually appropriate responses or text. This is achieved using a generative model like GPT-3 or BERT.

Mechanism: The generative model leverages the context provided by the retrieved documents to produce more accurate and relevant outputs, blending retrieval results with its generative capabilities.

Benefits of RAG:

RAG improves the accuracy of responses by grounding generated text in actual data, enhancing factual correctness. It also boosts contextual relevance by incorporating real-time information retrieval, ensuring that responses are pertinent to the query. RAG's flexibility allows it to be adapted for various NLP tasks, making it a versatile tool in the AI toolkit. Additionally, its ability to retrieve the latest information ensures that responses are dynamically updated and current.

Applications of RAG:

Question-answering systems: RAG can retrieve relevant documents to answer complex questions accurately.

Content creation: RAG assists in generating detailed and informative content for articles, reports, or creative writing.

Customer support: RAG can provide accurate and contextually relevant answers by retrieving the latest information from knowledge bases.

Search engines: RAG improves search results by providing detailed and accurate answers based on retrieved documents.

Implementation of RAG on Google Cloud

Google Cloud provides robust tools and frameworks for implementing RAG models, including Vertex AI and BigQuery. These platforms facilitate the integration of retrieval mechanisms with large language models (LLMs) , enabling the development of scalable and efficient RAG applications.

Vertex AI: It offers a comprehensive suite for building and deploying machine learning models, including support for RAG frameworks.

BigQuery: It allows efficient querying and retrieval of large datasets, providing a robust backend for the retrieval component of RAG models.

Key features of RAG on Google Cloud

Scalability: Google Cloud's infrastructure ensures that RAG models can handle large-scale data retrieval and processing.

Integration: Seamless integration with various data sources and APIs to facilitate comprehensive retrieval capabilities.

Customization: Tools and frameworks to customize RAG models according to specific business needs and applications.

Example:

Consider a RAG-based system designed to answer questions about historical events. When asked, "What were the key causes of World War II?" the system first retrieves documents or passages from a history database that discusses the causes of World War II. It then generates a comprehensive answer based on this retrieved information, ensuring that the response is both accurate and informative.

Example use case: Enhancing LLMs with RAG

By integrating RAG with Google Cloud's BigQuery, organizations can bolster the capabilities of their LLMs. For instance, a customer support application can use RAG to retrieve the latest policy documents from a database, ensuring that the LLM's responses are accurate and current.

Summary

RAG is an advanced method that combines the strengths of generative AI models with external information retrieval mechanisms to address several limitations inherent in generative models.
RAG addresses the limitations of generative AI models by retrieving relevant documents or data from a corpus before generating a response.
RAG incorporates a retrieval step to ensure that the generative model is grounded in actual data, thus reducing the risk of hallucinations.
RAG has several applications, including:
Question-answering systems.
Content creation.
Customer support.
Search engines.

The concept of Retrieval Augmented Generation (RAG) as a framework to improve the accuracy and relevance of large language models (LLMs).

Understanding LLM Challenges

LLMs can generate confident but incorrect answers, as illustrated by an anecdote about the number of moons in the solar system.
Two main issues are identified: lack of sourcing for answers and outdated information.
Retrieval Augmented Generation (RAG) Framework

RAG enhances LLMs by incorporating a content store for retrieving relevant information before generating responses.
This process allows the model to provide updated and sourced answers, reducing the likelihood of hallucination.
Benefits of RAG Implementation

RAG addresses the outdated information problem by allowing easy updates to the content store without retraining the model.
It also encourages the model to reference primary sources, improving answer reliability and enabling it to acknowledge when it does not know an answer.

The adoption of artificial intelligence (AI) in business operations, highlighting its benefits and the steps for effective integration.

Key Benefits of Adopting AI

AI enhances efficiency by automating repetitive tasks, allowing employees to focus on higher-value work.
It improves decision-making through data analysis, predicts market trends, and personalizes customer service.
Real-World Examples of AI in Business

Amazon uses AI for personalized recommendations, optimizing supply chains, and enhancing customer interactions through its voice assistant, Alexa.
Tesla employs AI in autonomous driving technology, improving safety and efficiency through real-time data analysis.
Steps to Adopt AI in Business

Define business goals by identifying specific problems AI can solve and engaging stakeholders.
Ensure data readiness by collecting, cleaning, and organizing data for AI algorithms.
Build AI capabilities by training employees, deploying AI solutions, and continuously monitoring and optimizing AI systems.

The adoption of artificial intelligence (AI) in business operations, highlighting its benefits and the steps for effective integration.

Key Benefits of Adopting AI

AI enhances efficiency by automating repetitive tasks, allowing employees to focus on higher-value work.
It improves decision-making through data analysis, predicts market trends, and personalizes customer service.
Real-World Examples of AI in Business

Amazon uses AI for personalized recommendations, optimizing supply chains, and enhancing customer interactions through its voice assistant, Alexa.
Tesla employs AI in autonomous driving technology, improving safety and efficiency through real-time data analysis.
Steps to Adopt AI in Business

Define business goals by identifying specific problems AI can solve and engaging stakeholders.
Ensure data readiness by collecting, cleaning, and organizing data for AI algorithms.
Build AI capabilities by training employees, deploying AI solutions, and continuously monitoring and optimizing AI systems.

The IBM AI ladder framework for adopting artificial intelligence (AI) in organizations.

IBM AI Ladder Framework

Collect: This initial stage involves gathering high-quality data from various sources using tools like IBM Cloud and Watsonx.data. Key actions include identifying data sources and ensuring data quality.
Organize: In this stage, data is cleaned, categorized, and made accessible for analysis. Tools such as IBM Cloud Pak for Data and IBM watsonx.governance assist in data management and governance.
AI Integration and the AI+ Approach

Analyze: AI's potential is harnessed through advanced analytics and machine learning models using tools like SPSS and Watson Machine Learning. Activities include developing predictive models and visualizing insights.
Infuse: This final stage integrates AI into daily operations to enhance decision-making and automate tasks, utilizing tools like IBM watsonx APIs and RPA.
Shifting Mindsets

The transition from a +AI to an AI+ approach emphasizes holistic integration of AI across all business functions, focusing on identifying high-value use cases and selecting appropriate technologies.
Continuous innovation and a robust data foundation are essential for effective AI implementation, ensuring compliance with data privacy regulations and adapting to evolving business challenges.

The various AI tools utilized by professionals across different industries to enhance productivity and efficiency.

AI Tools in Professional Settings

Microsoft Copilot assists research analysts by summarizing articles and streamlining tasks in applications like Word and Excel.
Google Gemini helps healthcare professionals create standardized patient intake forms, ensuring accuracy and efficiency.
Applications in Content Creation and Marketing

Synthesia enables marketers to produce engaging videos using digital avatars, simplifying video production.
QuillBot refines and condenses text for social media posts, while Grammarly checks grammar and style for clarity and consistency.
AI Tools for Language Learning and Customer Support

Duolingo and Google Translate support language learners and international business consultants in refining language skills and facilitating communication.
AI chatbots and Zendesk enhance customer support by providing instant responses and suggesting solutions based on inquiries.
Data Analysis and HR Applications

Tableau and Microsoft Power BI are used by data professionals for data visualization and analysis.
HR professionals utilize Culture Amp and IBM watsonx Orchestrate to improve employee engagement and streamline recruitment processes.

Career opportunities in the field of Artificial Intelligence (AI) and the skills needed to pursue them.

AI Careers Overview

Key AI roles include AI engineer, data scientist, robotics engineer, NLP engineer, AI application developer, and AI research scientist.
Many AI-related jobs do not require strong programming skills, such as AI ethicist, product manager, strategist, and marketing specialist.
Transitioning to an AI Career

Identify existing skills that can be leveraged, such as problem-solving and analytical thinking.
Learn core AI concepts like machine learning and natural language processing, with Python being a key programming language.
Staying Updated and Gaining Experience

Engage with AI professionals, attend conferences, and read publications to stay informed about trends.
Consider contributing to open-source projects or developing small AI projects to apply your skills practically.

The decision-making process between humans and artificial intelligence (AI) in various contexts, particularly in fraud detection systems.

Understanding Decision-Making in AI and Humans

The effectiveness of AI versus humans varies by task; AI excels in certain decisions while humans perform better in others.
In fraud detection, AI generates alerts for potentially fraudulent transactions, but many alerts are false positives, leading to an overwhelming workload for financial analysts.
Performance Curves of AI and Humans

AI typically shows a performance curve where high confidence scores correlate with accurate predictions, while humans perform better when AI is unsure.
At a 50% confidence level, humans can outperform AI by incorporating additional context and information.
Augmented Intelligence: A Combined Approach

Augmented intelligence combines human decision-making with AI assistance, leading to improved outcomes for many predictions.
The presentation of AI recommendations significantly impacts human decision-making, with forced displays potentially leading to automation bias.
Cognitive Bias and Trust in AI

Trust in AI recommendations can be affected by how information is presented, with humans often disregarding AI suggestions when accuracy percentages are provided.
Effective collaboration between humans and AI can enhance decision-making, provided cognitive biases are managed appropriately.

Brief Summary of the key concepts on Introduction to AI

Introduction to Artificial Intelligence (AI):

AI Agents: Software programs that interact with their environment, process data, and perform tasks autonomously to achieve human-defined goals.
Robotics: Involves designing and operating robots, including collaborative robots (cobots) that work alongside humans.
Generative AI: Uses models like Generative Adversarial Networks (GANs) to create content, analyze data, and enhance creativity in various fields.
AI in Business: Automates repetitive tasks, improves decision-making, and drives innovation, with steps for adoption including defining goals, data readiness, and monitoring systems.
Career Opportunities: Roles such as AI ethicist, product manager, and marketing specialist are emerging in the AI field.

Module 4

The ethical considerations and responsible use of AI, highlighting the importance of integrating ethical principles into AI development.

Ethical Concerns in AI

Data privacy and security are critical, requiring compliance with regulations like GDPR and CCPA to protect sensitive information.
Bias in AI systems can lead to discrimination; using diverse datasets and fairness-aware algorithms is essential to mitigate this issue.
Transparency and Accountability

Transparency in AI operations builds trust, necessitating clear communication about decision-making processes and data usage.
Establishing accountability is crucial; it should be clear who is responsible for AI decisions and how to address any harm caused.
Equity and Environmental Impact

Access to AI technology must be equitable to prevent exacerbating existing inequalities; efforts should be made to democratize AI.
The environmental impact of AI, particularly energy consumption, calls for the development of energy-efficient algorithms and sustainable practices.

the complexities and considerations surrounding generative AI, particularly in terms of copyright, privacy, accuracy, and ethics.

Copyright and Content Ownership

The sale of AI-generated artwork has raised questions about ownership rights, highlighting the complexities of intellectual property in AI.
Establishing clear guidelines and regulations is essential to balance the interests of AI developers, users, and society.
Privacy and Confidentiality

Generative AI models require large datasets, which may include sensitive information, necessitating strong data protection measures.
Private AI environments are increasingly used to ensure data confidentiality and prevent unauthorized access.
Accuracy and Hallucinations

Generative AI can produce inaccurate outputs, known as hallucinations, which can lead to misinformation.
Critical thinking and validation of AI-generated content are crucial to mitigate the risks of inaccuracies.
Ethical Considerations

AI systems can perpetuate biases from training data, making fairness and accountability vital in AI development.
The potential misuse of generative AI, such as creating deepfakes, underscores the need for responsible use and ethical guidelines.

the phenomenon of "hallucinations" in large language models (LLMs) and how they can produce inaccurate or fabricated information.

Understanding Hallucinations

Hallucinations are outputs from LLMs that deviate from factual accuracy, ranging from minor inconsistencies to complete fabrications.
They can be categorized into types such as sentence contradictions, prompt contradictions, factual errors, and nonsensical information.
Causes of Hallucinations

Data Quality: LLMs are trained on vast datasets that may contain inaccuracies or biases, leading to incorrect outputs.
Generation Method: The techniques used to generate text can introduce biases, affecting the balance between fluency and accuracy.
Input Context: The clarity and consistency of the input prompt significantly influence the accuracy of the generated response.
Minimizing Hallucinations

Provide Clear Prompts: Specific and detailed prompts help guide LLMs to produce more accurate outputs.
Use Active Mitigation Strategies: Adjusting settings like the temperature parameter can control the randomness of responses.
Employ Multi-shot Prompting: Offering multiple examples of desired outputs can help the model understand the expected context better.

A hallucination in large language models (LLMs) refers to outputs that deviate from factual accuracy or logical consistency. These can manifest in various ways, including:

Sentence Contradiction: Generating statements that contradict previous sentences (e.g., "The sky is blue today" followed by "The sky is green today").
Prompt Contradiction: Producing responses that contradict the input prompt (e.g., asking for a positive review and receiving a negative one).
Factual Errors: Providing incorrect information that is presented as fact (e.g., stating that Barack Obama was the first president of the United States).
Nonsensical Information: Including irrelevant or nonsensical details (e.g., mixing unrelated facts).
These hallucinations can range from minor inconsistencies to completely fabricated statements, making it essential to critically evaluate the outputs generated by LLMs.

To minimize hallucinations in large language models (LLMs), you can employ the following strategies:

Provide Clear and Specific Prompts:

Use detailed and precise input prompts to guide the model towards generating relevant and accurate outputs. For example, instead of asking, "What happened in World War II?" you could ask, "Can you summarize the major events of World War II, including the key countries involved and the primary causes of the conflict?"
Active Mitigation Strategies:

Adjust settings like the temperature parameter during text generation. A lower temperature produces more focused and conservative responses, while a higher temperature may lead to more creative but potentially less accurate outputs.
Multi-shot Prompting:
Provide multiple examples of the desired output format or context. This helps the model recognize patterns and understand user expectations more effectively, especially in tasks requiring specific output formats.
By implementing these strategies, you can enhance the accuracy and relevance of the responses generated by LLMs.

the ethical considerations and approaches to AI adopted by major organizations like IBM, Microsoft, and Google.

IBM's Pillars of Trust

IBM promotes ethical AI through five pillars: explainability, fairness, robustness, transparency, and privacy.
Toolkits like AI Explainability 360 and AI Fairness 360 support these pillars by enhancing understanding and mitigating bias in AI systems.
Microsoft's Approach to Ethical AI

Microsoft emphasizes human oversight in AI development through human-in-the-loop systems and continuous monitoring.
It conducts regular audits and uses algorithmic impact assessments to ensure compliance with ethical standards.
Google's AI Principles

Google's principles include being socially beneficial, avoiding bias, ensuring safety, and maintaining accountability.
A multidisciplinary review process assesses AI projects for ethical risks and compliance with these principles.

The importance of artificial intelligence (AI) governance in light of the rapid growth and deployment of AI systems.

AI Governance

AI governance involves a set of rules, standards, and processes to ensure the ethical and responsible development of AI systems.
It aims to minimize risks while maximizing the benefits of AI, such as reduced costs and improved efficiency.
Risks in AI Systems

AI systems can reflect human biases present in the training data, leading to biased outcomes.
Privacy and copyright infringements can occur if sensitive data is not properly managed during model training.
Challenges of AI Models

Black box models, while often more accurate, lack transparency, making it difficult to understand their decision-making processes.
Continuous monitoring is necessary to maintain the quality of AI models, as they can deteriorate over time due to changes in incoming data.
Regulatory Landscape

Organizations are developing regulations, such as the EU AI Act, to manage AI systems and ensure compliance, with penalties for non-compliance.
Effective governance is crucial for organizations to harness the full potential of AI while addressing associated risks.

The ethical considerations and guidelines for developing and using artificial intelligence (AI) systems.

Guidelines for Ethical AI

AI should enhance human intelligence rather than replace it.
Data ownership remains with the creator, emphasizing the importance of respecting customer data.
Design Thinking Activities

Dichotomy mapping helps identify features and their potential benefits, as well as any harm they may cause.
Considerations include data security, user accessibility, and the implications of data usage.
Implementing Solutions

Establish guardrails to ensure compliance with ethical guidelines, such as not selling data to advertisers.
Utilize diverse data sets for training AI to accommodate all users and mitigate bias, with tools like IBM's AI Fairness 360 for detecting bias in models.
Overall, the responsibility for ethical AI lies with everyone involved in its creation and use, ensuring it is safe and human-centered.

The ethical considerations and responsible use of artificial intelligence (AI), highlighting the importance of developing and deploying AI technologies in a manner that respects human rights and societal values.

Ethical Considerations in AI

Key principles include data privacy, bias and fairness, transparency, accountability, and human oversight.
Responsible AI use involves diverse datasets, human-in-the-loop systems, and ensuring accessibility for all.
Generative AI Challenges

Issues include copyright and ownership, privacy of user data, and the potential for misinformation through deepfakes.
Developers must prioritize fairness, accountability, and social benefits in AI development.
AI Governance and Risks

AI governance includes rules and standards to ensure ethical AI development.
Risks involve bias in models, privacy infringements, lack of transparency, and the need for continuous monitoring to maintain model performance.

A comprehensive overview of artificial intelligence (AI) and its applications.

Core Concepts of AI

AI performs tasks that typically require human intelligence, such as problem-solving and decision-making.
Generative AI creates new content using large language models and advanced algorithms.
Applications of AI

AI personalizes recommendations on platforms like Netflix and Spotify and enhances smart home devices.
Industries such as healthcare, finance, and retail utilize AI for data analysis, customer service, and product development.
Ethical Considerations in AI

Important ethical issues include data privacy, bias, transparency, and the need for human oversight.
Responsible use of AI is crucial for ensuring fairness and equity in its applications.

Glossary: Introduction to AI
Term	Definition
AI algorithms-Programming that tells the computer how to learn to operate on its own, thereby enabling it to attain artificial intelligence (AI).
AI-driven predictive maintenance systems-Using AI in production control, to monitor and detect missing materials or quality issues.
AI-driven robotics-Robots powered with AI that are augmented with a variety of sensors.
AI-driven energy management systems-Using AI in production control to manage and optimize energy production, distribution, and consumption within energy systems.
AI-powered quality control systems-Using AI in production control, to manage quality by enhancing efficiency, accuracy, and decision-making capabilities in various fields.
Augmented intelligence-Technology that enhances human capabilities by providing AI-powered tools and assistance.
Authorization-The process of determining whether an authenticated user has the right to perform an operation.
Automation-The techniques involving technology, programs, robotics, or processes to achieve minimal human intervention.
Big data stores-A larger, more complex data set, especially from new data sources.
Big data-A dynamic, large, and disparate volume of data being created by people,tools, and machines.
Biometric-Application of statistical analysis to biological data of individuals by means of unique physical characteristics.
Carbon footprints-The greenhouse gases produced by digital technology resources, devices, tools, and platforms.
Chatbots-Computer programs that simulate human conversation to provide information, answer questions, and perform tasks via text or voice interactions.
Claude models-Advanced AI language models, designed for natural language understanding and generation. They are similar to OpenAI's GPT series and are used in various applications such as chatbots, content creation, and other AI-driven tasks that require language processing capabilities.
Cloud computing-A technology that stores and uses data and services over the internet instead of keeping everything on your computer.
Cobots or collaborative robots-AI-driven robots that can work alongside humans in a shared workspace to enhance efficiency and increase productivity. Unlike traditional robots, cobots are designed to interact directly with people, making automation more accessible and versatile.
Cognitive-The term involves intellectual activities such as thinking, reasoning, and problem-solving.
Cognitive computing-A technology that can evaluate an individual's performance and offer personalized recommendations. It provides enhanced functionality, adaptability, and intelligence across various domains.
Cyberattack-An attempt targeting to damage a computer network, computer information system, or personal digital devices.
Cybersecurity-Protection of information technology of individuals and organizations from cyberattacks.
Data analysis-Process that involves cleaning, transforming, and modeling data to uncover useful information to aid business decision-making.
Data augmentation-The process of generating data from the existing data to train machine learning models.
Dashboards-Tool to present a bird's-eye view of the complete picture while also allowing you to drill down into the next level of information for each parameter. These are easy to comprehend for an average user, make collaboration easy between teams, and allow you to generate reports on the go.
Dall-E model-A multimodal model developed by OpenAI that exhibits the ability to generate images that precisely match the input text or prompt.
Data scientist-Data professionals who develop algorithms, build predictive models, and uncover patterns and trends in large data sets. They apply statistical analysis, especially inferential statistics, machine learning, and predictive modeling, to extract insights from data and make predictions.
Data readiness-The technique involves removing inconsistencies, filling gaps, and ensuring the data is relevant to the problem of the data.
Deepfake-Altering of an image or recording to misrepresent someone as doing or saying something that was not actually done or said.
Deep learning-Is a specialized subset of machine learning. Deep learning layers algorithms to create a neural network, which is an artificial replication of the brain's structure and functionality.
Digital landscape-The digital landscape encompasses all hardware, software, and content involved in digital advertising, which is where businesses and customers interact.
Digital data source-The digital location where the data is held in a data table, object, or other storage format.
E-commerce-A platform for buying and selling products and services, transmitting funds and data over the internet.
Edge computing-The practice of processing data closer to the source of generation rather than relying on centralized data centers.
Edge AI-A type of AI that lives on the device itself rather than relying on the cloud.
Electronic health records (EHRs)-A digital version of patient-centered records that makes information available in real time.
Encryption-A method of encoding data so that only authorized individuals can comprehend the information.
Generative AI-AI that is capable of creating new content (text, images, music, audio, and video) and responding to human conversations.
Generative AI: Variational autoencoders (VAEs)-VAEs are a type of generative AI model that works by transforming input data through encoding and decoding. They have three main parts: an encoder network, a latent space, and a decoder network.
Generative AI: Generative adversarial networks (GANs)-GANs involve two neural networks: the generator and the discriminator. The generator creates new data samples, and the discriminator checks if the data is real or fake.
Generative AI: Autoregressive models-Autoregressive models create data sequentially, considering the context of earlier generated elements. These models predict the next element in the sequence based on the previous one.
Generative AI: Transformers-Transformers are generally used in natural language processing (NLP) tasks. They consist of encoder and decoder layers, enabling the model to effectively generate text sequences or perform cross-language translations.
Generative AI: Unimodal models-Unimodal models process inputs and generate outputs within the same modality.
Generative AI: Multimodal models-Multimodal models handle inputs from one modality and produce outputs in a different modality.
Hallucinations-A phenomenon where AI can produce outputs that are inaccurate or completely fabricated. The phenomenon highlights the importance of critical thinking when using AI-generated content.
Internet of Things (IoT)-A network of physical devices connected to the internet that collect and share data for processing and analysis. These devices can be sensors, cameras, or other devices that generate data.
Inventory management-The process of tracking, controlling, and usage of inventory from purchase to the sale of the goods.
Interactive learning environment-A dynamic learning setup where learners actively engage with content, instructors, and peers through digital or physical interactions to enhance learning.
Large language model (LLM)-A type of AI program that can recognize and generate text. LLMs are pretrained on large amounts of data.
Machine learning-The branch of AI and computer science focuses on using data and algorithms to imitate how humans learn, gradually improving accuracy. It helps systems learn and improve at forecasting, much like how people learn from experience.
Machine learning models-Computer programs that aim to train the computers to identify patterns within new data and make predictions.
Machine learning engineers-Programmers who construct the algorithms, systems, models, and frameworks that enable machines to learn and perform functions independently and effectively.
Meta's Llama models-An accessible, open-source LLM designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas.
Natural language processing (NLP)-Machine learning technology that enables computers to interpret and comprehend human language.
Neural networks-Computational models that are influenced by the human brain's neural structure.
Neurons in AI-An artificial neural network consists of interconnected nodes known as neurons. The neurons take incoming data, like the human brain's neurological network, and learn to make decisions over time.
Neural network: Perceptron-The simplest type of artificial neural network consisting of only input and output layers.
Neural network: Feed-forward-A type of artificial neural network in which information flows in one direction, that is, in the forward direction. Each neuron in a layer receives input from neurons in the previous layer and then passes its output to neurons in the next layer.
Neural network: Deep feed-forward-A type of neural network that is similar to the feed-forward network with just more than one hidden layer.
Neural network: Modular-A type of neural network that combines two or more neural networks to arrive at the output.
Neural network: Convolutional neural network (CNN)-A type of neural network that is particularly well-suited for analyzing visual data.
Neural network: Recurrent neural networks (RNNs)-A type of neural network where the neurons in hidden layers receive an input with a specific delay in time. This allows the RNN to consider the context of the input.
IBM watsonx Assistant-A conversational AI solution that empowers a broader audience, including non-technical business users, to effortlessly build generative AI Assistants.
OpenAI-A private AI research laboratory that aims to develop and direct AI in ways that produce services.
Predictive analytics-A method that predicts future outcomes by using historical data combined with statistical modeling, data mining techniques, and machine learning.
Rehabilitation robots-Robots that help patients recover mobility and strength, offering personalized therapy.
Reinforcement learning-A category of machine learning technology that trains software to make decisions to achieve the most optimal results.
Robo-advisors-An online application that uses AI algorithms to offer automated, algorithm-driven investment suggestions, portfolio management, and financial planning services.
Robots-Machines with complex systems made up of several key components such as sensors, actuators, and controllers.
Robotics-A technology that involves designing, constructing, and operating robots, where the machines can perform tasks by themselves or with some help.
Robotic process automation (RPA)-A type of computer software that helps create, use, and control virtual robots. These robots act like humans when they work with digital systems and software.
Salesforce Marketing Cloud-An AI-driven marketing automation platform that uses machine learning algorithms to segment customers, target them with personalized messages, and optimize campaign performance.
Sentiment analysis-Processing digital text using AI to analyze and determine if the emotional tone of the message is positive, negative, or neutral.
Smart grids-An electricity network that uses digital technologies to monitor and manage the transport of electricity from sources to meet the varying electricity demands of end users.
Smart home-A technology that allows homeowners to control appliances, thermostats, lights, and other devices remotely using a smartphone or tablet through an internet connection.
Speech-to-text (STT)-A technology that changes spoken words into written text using neural networks.
Supervised learning-A category of machine learning technology that uses labeled datasets to train algorithms to predict outcomes and recognize patterns.
Strong AI or generalized AI-A type of AI system that replicates advanced human functions, such as reasoning, planning, and problem-solving.
Super AI or conscious AI-A type of AI system that is superintelligent by being self-aware and intelligent enough to surpass the cognitive abilities of humans.
Statistical analysis-The process of collecting large volumes of data and then using statistics and other data analysis techniques to identify trends, patterns, and insights.
Statistical methods-Methods useful to ensure that data is interpreted correctly and apparent relationships are meaningful and not just happening by chance.
Text-to-speech (TTS)-A type of assistive, "read aloud," technology that reads digital text aloud.
Unsupervised learning-A category of machine learning technology that uses algorithms to analyze and cluster unlabeled data sets.
Virtual assistance-AI-powered software applications designed to assist users with various tasks, such as setting reminders, providing information, and managing schedules.
Voice assistance-They can be interacted with using voice commands. Users can speak to these assistants to perform tasks, ask questions, and control other devices.
Video analytics-Use of advanced AI and machine learning algorithms to monitor, analyze, and manage large volumes of video.
Weak AI or narrow AI	AI that is focused on one narrow task.
